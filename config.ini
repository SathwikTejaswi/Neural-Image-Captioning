[EVAL]
Encoder_path = models/E2.ckpt
Decoder_path = models/D2.ckpt
vocab_path = data/vocab.pkl
embed_size = 256
hidden_size = 512
num_layers = 1
crop_size = 224
image_dir = data/val_resized2014
caption_path = data/annotations/captions_val2014.json
num_workers = 2
batch_size = 1

[EVAL2]
Encoder_path = models/v3_encoder-5-1000.ckpt
Decoder_path = models/v3_decoder-5-1000.ckpt
vocab_path = data/vocab.pkl
embed_size = 300
hidden_size = 50
num_layers = 3
crop_size = 224
image_dir = data/val_resized2014
caption_path = data/annotations/captions_val2014.json
num_workers = 2
batch_size = 1

[EVAL3]
Encoder_path = models/v3_encoder-1-3000.ckpt
Decoder_path = models/v3_decoder-1-3000.ckpt
vocab_path = data/vocab.pkl
embed_size = 512
hidden_size = 512
num_layers = 1
crop_size = 299
image_dir = data/val_resized2014
caption_path = data/annotations/captions_val2014.json
num_workers = 2
batch_size = 1

[TRAIN]
Encoder_path = None
Decoder_path = None
model_path = models/train
crop_size = 224
vocab_path = data/vocab.pkl
image_dir = data/resized2014
caption_path = data/annotations/captions_train2014.json
log_step = 10
save_step = 1000
embed_size = 256
hidden_size = 512
num_layers = 1
num_epochs = 5
batch_size = 128
num_workers = 2
learning_rate = 0.0001


[TRAIN2]
model_path = models/train2
Encoder_path = None
Decoder_path = None
crop_size = 224
vocab_path = data/vocab.pkl
image_dir = data/resized2014
caption_path = data/annotations/captions_train2014.json
log_step = 10
save_step = 1000
embed_size = 300
hidden_size = 50
num_layers = 3
num_epochs = 5
batch_size = 250
num_workers = 2
learning_rate = 0.0001

[TRAIN3]
model_path = models/train3
Encoder_path = None
Decoder_path = None
crop_size = 299
vocab_path = data/vocab.pkl
image_dir = data/resized2014
caption_path = data/annotations/captions_train2014.json
log_step = 10
save_step = 1000
embed_size = 512
hidden_size = 512
num_layers = 1
num_epochs = 5
batch_size = 128
num_workers = 2
learning_rate = 0.0001

[TRAIN4]
model_path = models/train4/
Encoder_path = None
Decoder_path = None
crop_size = 299
vocab_path = data/vocab.pkl
image_dir = data/resized2014
caption_path = data/annotations/captions_train2014.json
log_step = 10
save_step = 1000
embed_size = 512
hidden_size = 512
num_layers = 1
num_epochs = 5
batch_size = 128
num_workers = 2
learning_rate = 0.0001
